{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pathlib\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CLEAN_DATA_FOLDER = os.path.join(\"/Users\", \"mmackenzie\", \"Data\", \"peace-speech-project\", \"clean_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_countries(): \n",
    "    country_folders = glob.glob(os.path.join(CLEAN_DATA_FOLDER, \"*/\"))\n",
    "    return [pathlib.Path(x).parts[-1] for x in country_folders]\n",
    "\n",
    "def get_all_files(country: str, publisher=\"*\", year=\"*\"):\n",
    "    text_files = glob.glob(os.path.join(CLEAN_DATA_FOLDER, country, publisher, year, \"*.txt\"))\n",
    "    return [os.path.join(*pathlib.Path(x).parts[-4:]) for x in text_files]\n",
    "\n",
    "def clean_file_name(file_name):\n",
    "    id_, _, date = file_name[:-4].split(\"_\")\n",
    "    month = datetime.strptime(date, \"%d-%m-%y\").month\n",
    "    return (month, id_)\n",
    "\n",
    "def get_details_from_path(file_path):\n",
    "    parts = pathlib.Path(file_path).parts[-4:]\n",
    "    extra = clean_file_name(parts[-1])\n",
    "    names = [\"country\", \"publisher\", \"year\", \"month\", \"id\"]\n",
    "    return pd.Series(\n",
    "        (*parts[:-1], *extra),\n",
    "        index=names\n",
    "    )\n",
    "\n",
    "def get_text_from_file(file_path, path=CLEAN_DATA_FOLDER):\n",
    "    with open(os.path.join(path, file_path), \"r\", encoding=\"ISO-8859-1\") as f:\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        \n",
    "    title = lines[1].strip()\n",
    "    text = lines[-1].strip()\n",
    "        \n",
    "        \n",
    "    return pd.Series(\n",
    "        (title, text),\n",
    "        index=[\"title\", \"text\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:27<00:00,  1.35s/it]\n"
     ]
    }
   ],
   "source": [
    "countries = get_countries()\n",
    "article_paths = []\n",
    "for country in tqdm(countries):\n",
    "    article_paths.append(pd.Series(get_all_files(country)))\n",
    "    \n",
    "articles = pd.concat(article_paths).rename(\"path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mmackenzie/miniconda3/envs/peace/lib/python3.8/site-packages/tqdm/std.py:697: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "100%|██████████| 1415825/1415825 [06:49<00:00, 3459.54it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "path_details = articles.progress_apply(get_details_from_path).reset_index(drop=True)\n",
    "path_details = path_details.join(articles.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_counts = path_details.groupby(\n",
    "    [\"country\", \"publisher\", \"year\"]\n",
    ").size().rename(\"num_articles\").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>publisher</th>\n",
       "      <th>total_articles</th>\n",
       "      <th>sampled_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AU</td>\n",
       "      <td>2010</td>\n",
       "      <td>abc-local</td>\n",
       "      <td>248</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AU</td>\n",
       "      <td>2010</td>\n",
       "      <td>abc-online</td>\n",
       "      <td>2010</td>\n",
       "      <td>503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AU</td>\n",
       "      <td>2010</td>\n",
       "      <td>adnews</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AU</td>\n",
       "      <td>2010</td>\n",
       "      <td>architecture-and-design</td>\n",
       "      <td>108</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AU</td>\n",
       "      <td>2010</td>\n",
       "      <td>australian-personal-computer</td>\n",
       "      <td>193</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country  year                     publisher  total_articles  \\\n",
       "0      AU  2010                     abc-local             248   \n",
       "1      AU  2010                    abc-online            2010   \n",
       "2      AU  2010                        adnews               5   \n",
       "3      AU  2010       architecture-and-design             108   \n",
       "4      AU  2010  australian-personal-computer             193   \n",
       "\n",
       "   sampled_articles  \n",
       "0                62  \n",
       "1               503  \n",
       "2                 1  \n",
       "3                27  \n",
       "4                48  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CAP = 2500\n",
    "\n",
    "def get_num_samples(x, num_samples):\n",
    "    pct, max_ = x[\"pct\"], x[\"total_articles\"]\n",
    "    n = int(np.around(num_samples * pct))\n",
    "    if n > max_:\n",
    "        n = max_\n",
    "        \n",
    "    return n if n > 0 else 1\n",
    "\n",
    "sampled_counts = []\n",
    "for country, country_df in sample_counts.groupby(\"country\"):\n",
    "    \n",
    "    articles_by_year = country_df.groupby(\"year\")[\"num_articles\"].sum()\n",
    "    \n",
    "    num_to_sample = CAP\n",
    "#     num_to_sample = np.min([CAP, num_to_sample])\n",
    "    \n",
    "    # Going through each year at a time per country. \n",
    "    for year, year_df in country_df.groupby(\"year\"):\n",
    "        publishers = year_df.groupby(\"publisher\")[\"num_articles\"].sum().reset_index()\n",
    "        publishers[\"country\"] = country\n",
    "        publishers[\"year\"] = year\n",
    "        \n",
    "        publishers[\"pct\"] = publishers.num_articles / publishers.num_articles.sum()\n",
    "        publishers[\"total_articles\"] = publishers.num_articles\n",
    "        \n",
    "        publishers[\"sampled_articles\"] = publishers.apply(\n",
    "            get_num_samples, axis=1, num_samples=num_to_sample\n",
    "        )\n",
    "    \n",
    "        sampled_counts.append(publishers[[\n",
    "            \"country\", \"year\", \"publisher\", \"total_articles\", \"sampled_articles\"\n",
    "         ]].copy())\n",
    "        \n",
    "small_sample = pd.concat(sampled_counts)\n",
    "small_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_articles      1,415,825\n",
      "sampled_articles      483,870\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(small_sample[[\"total_articles\", \"sampled_articles\"]].sum().apply(lambda x: f\"{x:,d}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14288/14288 [01:04<00:00, 220.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>publisher</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2016</td>\n",
       "      <td>1194842</td>\n",
       "      <td>AU/9honey/2016/15732244_AU_04-12-16.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2016</td>\n",
       "      <td>1194843</td>\n",
       "      <td>AU/9honey/2016/16008320_AU_20-12-16.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2017</td>\n",
       "      <td>1194703</td>\n",
       "      <td>AU/9honey/2017/18481001_AU_16-05-17.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2017</td>\n",
       "      <td>1194637</td>\n",
       "      <td>AU/9honey/2017/22359622_AU_21-12-17.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2017</td>\n",
       "      <td>1194691</td>\n",
       "      <td>AU/9honey/2017/21232891_AU_21-10-17.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country publisher  year       id                                     path\n",
       "0      AU    9honey  2016  1194842  AU/9honey/2016/15732244_AU_04-12-16.txt\n",
       "1      AU    9honey  2016  1194843  AU/9honey/2016/16008320_AU_20-12-16.txt\n",
       "2      AU    9honey  2017  1194703  AU/9honey/2017/18481001_AU_16-05-17.txt\n",
       "3      AU    9honey  2017  1194637  AU/9honey/2017/22359622_AU_21-12-17.txt\n",
       "4      AU    9honey  2017  1194691  AU/9honey/2017/21232891_AU_21-10-17.txt"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "def sample_paths(x):\n",
    "    country, publisher, year = x.iloc[0, :3]\n",
    "    to_sample = small_sample.loc[\n",
    "        (small_sample.country == country) &\n",
    "        (small_sample.publisher == publisher) &\n",
    "        (small_sample.year == year)\n",
    "    ].iloc[0, -1]\n",
    "    \n",
    "    return x.sample(to_sample)[\"path\"]\n",
    "    sys.exit()\n",
    "\n",
    "paths_to_sample = path_details.groupby([\"country\", \"publisher\", \"year\"]).progress_apply(sample_paths)\n",
    "paths_to_sample = paths_to_sample.reset_index().rename(columns={\"level_3\": \"id\"})\n",
    "paths_to_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 483870/483870 [40:53<00:00, 197.22it/s]   \n"
     ]
    }
   ],
   "source": [
    "text = paths_to_sample.path.progress_apply(get_text_from_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_to_sample.join(text).to_csv(\"../data/medium_sample_text.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>publisher</th>\n",
       "      <th>year</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2016</td>\n",
       "      <td>1194839</td>\n",
       "      <td>meghan markle target cruel topless photo leak ...</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2017</td>\n",
       "      <td>1194642</td>\n",
       "      <td>world 's big seafood market early bird get wor...</td>\n",
       "      <td>431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2017</td>\n",
       "      <td>1194719</td>\n",
       "      <td>acne try tell century traditional chinese medi...</td>\n",
       "      <td>432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2017</td>\n",
       "      <td>1194646</td>\n",
       "      <td>miss world finalist protect girl reveal rape f...</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AU</td>\n",
       "      <td>9honey</td>\n",
       "      <td>2017</td>\n",
       "      <td>1194639</td>\n",
       "      <td>guy go holiday complete stranger place friend ...</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country publisher  year       id  \\\n",
       "0      AU    9honey  2016  1194839   \n",
       "1      AU    9honey  2017  1194642   \n",
       "2      AU    9honey  2017  1194719   \n",
       "3      AU    9honey  2017  1194646   \n",
       "4      AU    9honey  2017  1194639   \n",
       "\n",
       "                                                text  n_words  \n",
       "0  meghan markle target cruel topless photo leak ...      181  \n",
       "1  world 's big seafood market early bird get wor...      431  \n",
       "2  acne try tell century traditional chinese medi...      432  \n",
       "3  miss world finalist protect girl reveal rape f...      122  \n",
       "4  guy go holiday complete stranger place friend ...      146  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = paths_to_sample.join(text)[[\"country\", \"publisher\", \"year\", \"id\", \"text\"]]\n",
    "text_df[\"n_words\"] = text_df.text.str.split().str.len()\n",
    "\n",
    "text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-c48937b106f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lexicon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"term\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlexicons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexicons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mlexicons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_words\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexicons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c48937b106f7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lexicon\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"term\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mlexicons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mread_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexicons\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mlexicons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_words\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexicons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c48937b106f7>\u001b[0m in \u001b[0;36mread_lexicon\u001b[0;34m(name, base_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEXICON_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"enh_{name}_lexicon.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"term\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mlexicon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lexicon\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mlexicon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"term\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/peace/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    871\u001b[0m                     \u001b[0;31m# AttributeError for IntervalTree get_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/peace/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/peace/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Too many indexers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 raise ValueError(\n",
      "\u001b[0;32m~/miniconda3/envs/peace/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1350\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/peace/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1437\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[0;31m# -------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "LEXICON_DIR = os.path.join('..','lexicons')\n",
    "lexicons = [\"peace\", \"conflict\", \"resilience\"]\n",
    "\n",
    "def read_lexicon(name, base_dir=LEXICON_DIR):\n",
    "    path = os.path.join(base_dir, f\"enh_{name}_lexicon.xlsx\")\n",
    "    lexicon = pd.read_excel(path).iloc[:, 1].rename(\"term\").to_frame()\n",
    "    lexicon[\"lexicon\"] = name\n",
    "    lexicon[\"term\"] = lexicon.term.str.lower()\n",
    "    return lexicon[[\"lexicon\", \"term\"]]\n",
    "\n",
    "lexicons = pd.concat([read_lexicon(os.path.join(p)) for p in lexicons]).reset_index(drop=True)\n",
    "lexicons[\"num_words\"] = lexicons.term.str.split().str.len()\n",
    "\n",
    "single_word_lexicon = lexicons[lexicons.num_words == 1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
