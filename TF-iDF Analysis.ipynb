{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from utils.reading import read_all_files, get_text\n",
    "from utils.common import get_country_labels, get_society_label\n",
    "\n",
    "CLEAN_DATA_FOLDER = os.path.join(\"/Users\", \"mmackenzie\", \"Data\", \"peace-speech-project\", \"clean_sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding files...: 100%|██████████| 3/3 [00:05<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "274,550 articles found.\n",
      "Sampling 20000 random articles.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting details: 100%|██████████| 20000/20000 [00:06<00:00, 3167.40it/s]\n",
      "Getting text: 100%|██████████| 20000/20000 [02:09<00:00, 154.44it/s]\n"
     ]
    }
   ],
   "source": [
    "articles = read_all_files(path=CLEAN_DATA_FOLDER, countries=[\"CA\", \"IN\", \"PK\"], sample=20_000)\n",
    "articles_with_text = get_text(articles, path=CLEAN_DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 9)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Peaceful       8101\n",
       "Other          7864\n",
       "Nonpeaceful    4035\n",
       "Name: society, dtype: int64"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_labels = get_country_labels()\n",
    "articles_with_text[\"society\"] = articles_with_text.country.apply(get_society_label, country_labels=country_labels)\n",
    "\n",
    "print(articles_with_text.shape)\n",
    "articles_with_text.society.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def combine_text_by_group(text_df, group):\n",
    "    by_group = text_df.groupby(group).text.apply(lambda x: x.str.cat(sep = \"\\n\\n\"))\n",
    "    return by_group.reset_index()\n",
    "    \n",
    "def fit_tfidf(text, binary=True, min_df=5, max_df=20):\n",
    "    tfidf = TfidfVectorizer(stop_words=\"english\", \n",
    "                            binary=binary, min_df=min_df, max_df=max_df)\n",
    "    tfidf.fit(text)\n",
    "    \n",
    "    return tfidf\n",
    "\n",
    "def get_top_n_terms(text, tfidf, n=10):\n",
    "    response = tfidf.transform(text)\n",
    "    top_values = np.partition(response.toarray(), -n, axis=1)[:, -n:][:, ::-1]\n",
    "\n",
    "    terms = np.array(tfidf.get_feature_names())\n",
    "    tfidf_sorting = np.argsort(-response.toarray())\n",
    "    top_n_by_doc_idx = tfidf_sorting[:, :n]\n",
    "    \n",
    "    top_terms = terms[top_n_by_doc_idx]\n",
    "    \n",
    "    return np.around(top_values, 4), top_terms\n",
    "\n",
    "def create_top_n_terms_df(text_df, top_terms, top_values):\n",
    "    df = text_df.copy()\n",
    "    \n",
    "    df[\"combined\"] = np.stack((top_terms,top_values), axis=2).tolist()\n",
    "    \n",
    "    df_long = df.explode(\"combined\")\n",
    "    df_long[['term', 'value']] = pd.DataFrame(df_long['combined'].tolist(), index=df_long.index) \n",
    "    \n",
    "    return df_long.drop(\"combined\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By Society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.82 s, sys: 125 ms, total: 4.94 s\n",
      "Wall time: 4.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "df = combine_text_by_group(articles_with_text, [\"society\"])\n",
    "tfidf = fit_tfidf(df.text, min_df=1, max_df=2, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.34 s, sys: 93.4 ms, total: 4.43 s\n",
      "Wall time: 4.46 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "top_values, top_terms = get_top_n_terms(df.text, tfidf, 20)\n",
    "top_terms_df = create_top_n_terms_df(df.drop(\"text\", axis=1), top_terms, top_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['lahore', 'balochistan', 'peshawar', 'bhutto', 'zardari', 'asif',\n",
       "        'musharraf', 'mqm', 'rehman', 'rupee', 'fbr', 'pakhtunkhwa',\n",
       "        'chaudhry', 'nisar', 'quetta', 'benazir', 'tehreek', 'wapda',\n",
       "        'urdu', 'baloch'],\n",
       "       ['tnn', 'rupee', 'ishant', 'dhoni', 'gambhir', 'telangana',\n",
       "        'bcci', 'odi', 'goa', 'haryana', 'sebi', 'chhattisgarh', 'nagar',\n",
       "        'rohit', 'jharkhand', 'madhya', 'shiv', 'mishra', 'kejriwal',\n",
       "        'bse'],\n",
       "       ['nhl', 'quebec', 'edmonton', 'manitoba', 'winnipeg', 'rcmp',\n",
       "        'toolon', 'trudeau', 'uviews', 'scotia', 'harper', 'halifax',\n",
       "        'oiler', 'ont', 'saskatchewan', 'sudbury', 'cannabis', 'puck',\n",
       "        'canuck', 'brunswick']], dtype='<U40')"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
