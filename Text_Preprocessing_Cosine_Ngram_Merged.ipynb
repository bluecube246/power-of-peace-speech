{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import datetime\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_folder_for_processing = 'sampled_raw'\n",
    "Country_to_process = 'IN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PK, US, HK, GH, NG, KE, NZ, BD, IN, LK, CA, ZA, SG, PH, GB, MY, AU, IE, JM, TZ, "
     ]
    }
   ],
   "source": [
    "base_dir = os.path.join(os.getcwd(), 'data', name_of_folder_for_processing) \n",
    "countries_list = os.listdir(base_dir) # dir is your directory path\n",
    "summary = []\n",
    "\n",
    "for con in countries_list:\n",
    "    if(os.path.isdir(os.path.join(base_dir,con))):\n",
    "        print(con, end=\", \", flush=True)\n",
    "        publishers = os.listdir(os.path.join(base_dir,con))\n",
    "        for pub in publishers:\n",
    "            subdir = os.path.join(base_dir, con, pub)\n",
    "            length = sum([len(files) for r, d, files in os.walk(subdir)])\n",
    "            summary.append([con, pub, length])\n",
    "\n",
    "summary_df = pd.DataFrame(summary, columns = ['country','publisher','count'])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>publisher</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PK</td>\n",
       "      <td>oyeyeah-com</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PK</td>\n",
       "      <td>nation-com-pk</td>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PK</td>\n",
       "      <td>business-recorder-press-release-registration-blog</td>\n",
       "      <td>1390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PK</td>\n",
       "      <td>paktribune-com</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PK</td>\n",
       "      <td>aaj-tv-press-release-blog</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>TZ</td>\n",
       "      <td>goal-com</td>\n",
       "      <td>1315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2864</th>\n",
       "      <td>TZ</td>\n",
       "      <td>rapid-tv-news</td>\n",
       "      <td>1632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2865</th>\n",
       "      <td>TZ</td>\n",
       "      <td>ippmedia-com</td>\n",
       "      <td>1115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>TZ</td>\n",
       "      <td>daily-news-the-national-newspaper</td>\n",
       "      <td>707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>TZ</td>\n",
       "      <td>daily-news-the-national-newspaper-press-releas...</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2868 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     country                                          publisher  count\n",
       "0         PK                                        oyeyeah-com    711\n",
       "1         PK                                      nation-com-pk    806\n",
       "2         PK  business-recorder-press-release-registration-blog   1390\n",
       "3         PK                                     paktribune-com    337\n",
       "4         PK                          aaj-tv-press-release-blog    143\n",
       "...      ...                                                ...    ...\n",
       "2863      TZ                                           goal-com   1315\n",
       "2864      TZ                                      rapid-tv-news   1632\n",
       "2865      TZ                                       ippmedia-com   1115\n",
       "2866      TZ                  daily-news-the-national-newspaper    707\n",
       "2867      TZ  daily-news-the-national-newspaper-press-releas...   1037\n",
       "\n",
       "[2868 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = summary_df.groupby(['country'])['count'].transform(max) == summary_df['count']\n",
    "# max_table = summary_df[idx]\n",
    "# max_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, IN, "
     ]
    }
   ],
   "source": [
    "overall_dict = {}\n",
    "\n",
    "for index, row in summary_df.iterrows():\n",
    "    country, publisher = row[0], row[1]\n",
    "    if country != Country_to_process:\n",
    "        pass\n",
    "    else:\n",
    "        \n",
    "        print(country, end=\", \", flush=True)\n",
    "        directory = os.path.join(base_dir, country, publisher)\n",
    "        df_dict = {\"id\": [], \"title\": [], \"publisher\": [], \"article_text\":[],\"url\":[], \"path\":[]}\n",
    "        if not os.path.isdir(directory):\n",
    "            pass\n",
    "        else:\n",
    "            for entry in os.scandir(directory):\n",
    "                if(os.path.isdir(entry)):\n",
    "                    for entry_2 in os.scandir(entry.path):\n",
    "                        try:\n",
    "                            with open(entry_2, \"r\") as f:\n",
    "                                article_id = f.readline().strip()\n",
    "                                article_title = f.readline().strip()\n",
    "                                publisher = f.readline().strip()\n",
    "                                url = f.readline()\n",
    "                                f.readline()\n",
    "                                article_text = f.readline().strip()\n",
    "                                df_dict['url'].append(url)\n",
    "                                df_dict['id'].append(article_id)\n",
    "                                df_dict['title'].append(article_title)\n",
    "                                df_dict['publisher'].append(publisher)\n",
    "                                df_dict['article_text'].append(article_text)\n",
    "                                df_dict['path'].append(entry_2)\n",
    "                        except:\n",
    "                            pass\n",
    "            sample_df = pd.DataFrame.from_dict(df_dict)\n",
    "            #sample_df = sample_df.sample(n=5, random_state=1)\n",
    "\n",
    "            if country not in overall_dict.keys():\n",
    "                overall_dict[country] = {}\n",
    "            overall_dict[country][publisher] = sample_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a clean function data\n",
    "def clean_text(text):\n",
    "    return text.strip()\n",
    "\n",
    "def text_initial_preproecss(df):\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df[~pd.to_numeric(df['id'], errors='coerce').isnull()]\n",
    "    df[\"id\"] = df[\"id\"].astype(int)\n",
    "    df['article_text'] = df.article_text.str.lower()\n",
    "    df[\"article_text\"] = df[\"article_text\"].apply(clean_text)\n",
    "    df['text'] = df['article_text']\n",
    "    reports = df\n",
    "    \n",
    "    # Puncutation preprocesing\n",
    "    reports['text'] = reports.text.str.replace('{', '')\n",
    "    reports['text'] = reports.text.str.replace('}', '')\n",
    "    reports['text'] = reports.text.str.replace(\"\\n\", '')\n",
    "    reports['text'] = reports.text.str.rstrip(\"\\n\") #remove empty lines\n",
    "    reports['text'] = reports.text.str.replace(\"@ @ @ @ @ @ @ @ @ @ \", '')\n",
    "    reports['text'] = reports.text.str.replace(\" @\", '')\n",
    "    reports['text'] = reports.text.str.replace(\" '\", \"'\")\n",
    "    reports['text'] = reports.text.str.replace(\"\\\"\", \"\")\n",
    "    reports['text'] = reports.text.str.replace(\",\", \"\")\n",
    "    reports['text'] = reports.text.str.replace(\"(\", \"\")\n",
    "    reports['text'] = reports.text.str.replace(\")\", \"\")\n",
    "    reports['text'] = reports.text.str.replace(\" <p>\", \".\")\n",
    "    reports['text'] = reports.text.str.replace(\" <h>\", \".\")\n",
    "    reports['text'] = reports.text.str.replace(\"<p>\", \"\")\n",
    "    reports['text'] = reports.text.str.replace(\"<h>\", \"\")\n",
    "    reports['text'] = reports.text.str.replace('<', '')\n",
    "    reports['text'] = reports.text.str.replace('>', '')\n",
    "    reports['text'] = reports.text.str.replace(\":\", \"\")\n",
    "    reports['text'] = reports.text.str.replace(\"?\", \".\")\n",
    "    reports['text'] = reports.text.str.replace(\"!\", \".\")\n",
    "    reports['text'] = reports.text.str.replace(r\"\\.\\s[\\.\\s]+\", \". \") #converting . . to .\n",
    "    reports['text'] = reports.text.str.replace(r\"\\.+\", \".\") #converting ... to .\n",
    "    reports['text'] = reports.text.str.replace(\"--\", \"\") \n",
    "    reports['text'] = reports.text.str.replace(\"-\", \" \")\n",
    "    reports['text'] = reports.text.str.replace(\" +\", \" \")\n",
    "    reports['text'] = reports.text.str.replace(\" n't\", \"n't\")\n",
    "    \n",
    "    return reports.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "os_name = platform.system()\n",
    "\n",
    "def save_file(report,base):\n",
    "#     print(report)\n",
    "#     print(type(report))\n",
    "    report = report.squeeze()\n",
    "        \n",
    "    full_path = report.publisher\n",
    "    file_name = str(report.id) + \".txt\"\n",
    "    \n",
    "    if os_name == 'Windows':\n",
    "        \n",
    "        base_dir = 'data\\preprocessed\\\\{}'.format(base)\n",
    "        con = report.path.path.split('{0}\\\\'.format(name_of_folder_for_processing))[1]\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        base_dir = 'data/preprocessed/{}'.format(base)\n",
    "        con = report.path.path.split('/{0}/'.format(name_of_folder_for_processing))[1]\n",
    "    \n",
    "    path = os.path.join(base_dir,con)\n",
    "    \n",
    "    full_path = os.path.dirname(path)\n",
    "    file_name = os.path.basename(path)\n",
    "\n",
    "    pathlib.Path(full_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with codecs.open(f\"{full_path}/{file_name}\", \"w\", encoding = \"utf-8\") as f:\n",
    "        id_number = str(report.id) if not pd.isna(report.id) else \"\"\n",
    "        title = report.title if not pd.isna(report.title) else \"\"\n",
    "        website = report.publisher if not pd.isna(report.publisher) else \"\"\n",
    "        url = report.url if not pd.isna(report.url) else \"\"\n",
    "        f.writelines([id_number, \"\\n\",\n",
    "                      title, \"\\n\",\n",
    "                      website, \"\\n\",\n",
    "                      url, \"\\n\\n\", report.text])\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_preprocess(df):\n",
    "    time_taken = []\n",
    "    \n",
    "#     reports_temp = df[df['publisher']==article][['id', 'text','text_vect']].copy()\n",
    "    if len(df) > 500:\n",
    "        reports_temp = df.sample(n=500, random_state=1).copy()\n",
    "    else:\n",
    "        reports_temp = df.copy()\n",
    "        \n",
    "    reports_temp['text_vect'] = reports_temp.text.apply(sent_tokenize)\n",
    "#     display(reports_temp.head(3))\n",
    "    word_vectorizer = CountVectorizer(ngram_range=(5,5), stop_words=[])\n",
    "    flatten = [item for sublist in reports_temp['text_vect'] for item in sublist]\n",
    "    sparse_matrix = word_vectorizer.fit_transform(flatten)\n",
    "\n",
    "    frequency = sum(sparse_matrix).toarray()[0]\n",
    "    frequency_df = pd.DataFrame(frequency, index=word_vectorizer.get_feature_names(),columns = ['frequency']).sort_values(by=['frequency'],ascending=False)\n",
    "\n",
    "    freq_above_10 = frequency_df[frequency_df['frequency'] >= max((len(reports_temp)/10),3)] #len(MAXSIZE)/2\n",
    "    phrase_list = list(freq_above_10.index)\n",
    "\n",
    "    sentences_to_remove = []\n",
    "\n",
    "    for phrase in phrase_list:\n",
    "#         removing_sent = set([sent for sent in flatten if phrase in sent])\n",
    "        removing_sent = set([sent for sent in flatten if ((phrase in sent) or (len(sent.split()) < 4))])\n",
    "\n",
    "        for sent in removing_sent:\n",
    "            sentences_to_remove.append(sent)\n",
    "\n",
    "    if(\".\" in sentences_to_remove):\n",
    "        while(\".\" in sentences_to_remove):\n",
    "            sentences_to_remove.remove(\".\")\n",
    "    if(\" .\" in sentences_to_remove):\n",
    "        while(\" .\" in sentences_to_remove):\n",
    "            sentences_to_remove.remove(\" .\")\n",
    "\n",
    "#    print(sentences_to_remove)\n",
    "    sentences_to_remove = list(set(sentences_to_remove))\n",
    "    sentences_to_remove.sort(key= len, reverse = True)  # sort starting by largest sentence, in case smaller sentence get chosen beforehand\n",
    "\n",
    "    for sent in sentences_to_remove:\n",
    "        df.text = df.text.apply(lambda x: str(x).replace(sent, \"\"))\n",
    "\n",
    "#     print(sentences_to_remove)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_preprocess(df, model, threshold):\n",
    "\n",
    "    sent_removed = []\n",
    "    time_taken = []\n",
    "    new_txts = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        \n",
    "        idx_remove = []\n",
    "        orig_txt = df['text'].iloc[i]\n",
    "        sent_text = nltk.sent_tokenize(orig_txt) #Sentence Segmentation\n",
    "        docu_embeddings = model.encode(orig_txt) #Document Embedding\n",
    "        sentence_embeddings_ = model.encode(sent_text) #Sentence Embedding\n",
    "\n",
    "        for k in range(len(sentence_embeddings_)):\n",
    "            \n",
    "            sim = cosine(docu_embeddings, sentence_embeddings_[k])\n",
    "\n",
    "            if sim > threshold:\n",
    "                idx_remove.append(k)\n",
    "\n",
    "        idx_remove.sort()\n",
    "        idx_remove.reverse()\n",
    "\n",
    "        for j in idx_remove:\n",
    "            sent_removed.append(sent_text[j])\n",
    "            del sent_text[j]\n",
    "\n",
    "        new_txt = ' '.join(sent_text)\n",
    "        new_txts.append(new_txt)\n",
    "        \n",
    "    df['text'] = new_txts\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving original file, to do comparison later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b9e422ef6ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_initial_preproecss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'orig'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   7545\u001b[0m             \u001b[0mkwds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7546\u001b[0m         )\n\u001b[0;32m-> 7547\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7549\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    282\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m                     \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5201ce6c7ffb>\u001b[0m in \u001b[0;36msave_file\u001b[0;34m(report, base)\u001b[0m\n\u001b[1;32m     36\u001b[0m                       \u001b[0mwebsite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                       url, \"\\n\\n\", report.text])\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for keys1 in overall_dict.keys():\n",
    "    for keys2 in overall_dict[keys1].keys():\n",
    "        df = overall_dict[keys1][keys2]\n",
    "        df = text_initial_preproecss(df)\n",
    "\n",
    "        df.apply(save_file,args=('orig',),axis=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hojinlee/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/hojinlee/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hojinlee/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "from textblob import TextBlob, Word\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopwords.words('english')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def remove_digits(text):\n",
    "    text = re.sub(r'\\d+','', text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    word_tokens = word_tokenize(text) \n",
    "    filtered_sentence = [w for w in word_tokens if not w in string.punctuation]\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def nltk_lemmatizer(text):\n",
    "    '''\n",
    "    lemmatize each word in sentence if the word is not punctuation\n",
    "    Below version is just for ease of understanding\n",
    "    '''\n",
    "    words = word_tokenize(text)\n",
    "    lem_words = []\n",
    "    for word in words:\n",
    "        if word not in string.punctuation:\n",
    "            lem_words.append(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "    return ' '.join(lem_words)\n",
    "\n",
    "def lemmatize_with_postag(text):\n",
    "    sent = TextBlob(text)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Methodology: ngram\n",
      "Running time, country: IN, publisher: News Nation, punct_t: 0:00:00.073108, text_proc_t: 0:00:11.222836, total_t: 0:00:11.295944\n",
      "Running time, country: IN, publisher: Gulte, punct_t: 0:00:00.031223, text_proc_t: 0:00:02.036926, total_t: 0:00:02.068149\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b898d9b486e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mN_gram_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mngram_preprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fb0feb8fca85>\u001b[0m in \u001b[0;36mngram_preprocess\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msparse_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mfrequency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mfrequency_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrequency\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mword_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'frequency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frequency'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/csr.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0mi0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cosine_process = False\n",
    "N_gram_process = True\n",
    "merged_process = True if cosine_process and N_gram_process else False\n",
    "cosine_threshold=0.95\n",
    "\n",
    "time_taken = []\n",
    "text_list = []\n",
    "\n",
    "if merged_process:\n",
    "    base='merged'\n",
    "elif cosine_process:\n",
    "    base='cosine'\n",
    "else:\n",
    "    base='ngram'\n",
    "\n",
    "print(\"Methodology: {}\".format(base))\n",
    "  \n",
    "for keys1 in overall_dict.keys():\n",
    "    if keys1 != Country_to_process:\n",
    "        pass\n",
    "    else:\n",
    "        for keys2 in overall_dict[keys1].keys(): # i.e. 'US', \"CA\"\n",
    "\n",
    "            if keys1 =='IN':\n",
    "                a = datetime.datetime.now()\n",
    "\n",
    "                df = overall_dict[keys1][keys2]\n",
    "                df = text_initial_preproecss(df)\n",
    "\n",
    "                b = datetime.datetime.now()\n",
    "\n",
    "                if merged_process:\n",
    "                    df = ngram_preprocess(df)\n",
    "                    df = cosine_preprocess(df,model,cosine_threshold)\n",
    "\n",
    "                elif cosine_process:\n",
    "                    df = cosine_preprocess(df,model,cosine_threshold)\n",
    "\n",
    "                elif N_gram_process:\n",
    "                    df = ngram_preprocess(df)\n",
    "\n",
    "\n",
    "                df.apply(save_file,args = (base,),axis=1)\n",
    "\n",
    "                df.text = df.text.apply(remove_stopwords)\n",
    "                df.text = df.text.apply(remove_punct)\n",
    "                df.text = df.text.apply(remove_digits)\n",
    "                df.text = df.text.apply(lemmatize_with_postag)\n",
    "\n",
    "                #saving files\n",
    "                df.apply(save_file,args = ('lemmatized',),axis=1)\n",
    "\n",
    "                c = datetime.datetime.now()\n",
    "\n",
    "                print(\"Running time, country: {}, publisher: {}, punct_t: {}, text_proc_t: {}, total_t: {}\".format(keys1,keys2, b-a, c-b, c-a))\n",
    "                time_taken.append(c-a)\n",
    "\n",
    "print(np.mean(time_taken))\n",
    "print(np.sum(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_process = True\n",
    "# N_gram_process = False\n",
    "# merged_process = True if cosine_process and N_gram_process else False\n",
    "# cosine_threshold=0.95\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# time_taken = []\n",
    "# text_list = []\n",
    "\n",
    "# if merged_process:\n",
    "#     base='merged'\n",
    "# elif cosine_process:\n",
    "#     base='cosine'\n",
    "# else:\n",
    "#     base='ngram'\n",
    "\n",
    "# print(\"Methodology: {}\".format(base))\n",
    "  \n",
    "# for keys in overall_dict.keys(): # i.e. 'US', \"CA\"\n",
    "    \n",
    "#     a = datetime.datetime.now()\n",
    "    \n",
    "#     df = overall_dict[keys]   \n",
    "#     df = text_initial_preproecss(df)\n",
    "    \n",
    "#     b = datetime.datetime.now()\n",
    "    \n",
    "# #     df.apply(save_file_initial,args = (base,),axis=1)\n",
    "\n",
    "# #     text_list.append(df.iloc[1])\n",
    "#     if merged_process:\n",
    "#         df = ngram_preprocess(df)\n",
    "#         df = cosine_preprocess(df,model,cosine_threshold)\n",
    "\n",
    "#     elif cosine_process:\n",
    "#         df = cosine_preprocess(df,model,cosine_threshold)\n",
    "\n",
    "#     elif N_gram_process:\n",
    "#         df = ngram_preprocess(df)\n",
    "\n",
    "    \n",
    "#     #saving files\n",
    "#     df.apply(save_file,args = (base,),axis=1)\n",
    "\n",
    "#     c = datetime.datetime.now()\n",
    "    \n",
    "#     print(\"Running time, country: {}, punct_t: {}, text_proc_t: {}, total_t: {}\".format(keys, b-a, c-b, c-a))\n",
    "#     time_taken.append(c-a)\n",
    "\n",
    "# print(np.mean(time_taken))\n",
    "# print(np.sum(time_taken))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine_process = True\n",
    "# N_gram_process = True\n",
    "# merged_process = True if cosine_process and N_gram_process else False\n",
    "# cosine_threshold=0.95\n",
    "\n",
    "# model = SentenceTransformer('sentence-transformers/bert-base-nli-stsb-mean-tokens')\n",
    "\n",
    "# time_taken = []\n",
    "# text_list = []\n",
    "\n",
    "# if merged_process:\n",
    "#     base='merged'\n",
    "# elif cosine_process:\n",
    "#     base='cosine'\n",
    "# else:\n",
    "#     base='ngram'\n",
    "\n",
    "# print(\"Methodology: {}\".format(base))\n",
    "  \n",
    "# for keys in overall_dict.keys(): # i.e. 'US', \"CA\"\n",
    "    \n",
    "#     a = datetime.datetime.now()\n",
    "    \n",
    "#     df = overall_dict[keys]   \n",
    "#     df = text_initial_preproecss(df)\n",
    "    \n",
    "#     b = datetime.datetime.now()\n",
    "    \n",
    "# #     df.apply(save_file_initial,args = (base,),axis=1)\n",
    "\n",
    "# #     text_list.append(df.iloc[1])\n",
    "#     if merged_process:\n",
    "#         df = ngram_preprocess(df)\n",
    "#         df = cosine_preprocess(df,model,cosine_threshold)\n",
    "\n",
    "#     elif cosine_process:\n",
    "#         df = cosine_preprocess(df,model,cosine_threshold)\n",
    "\n",
    "#     elif N_gram_process:\n",
    "#         df = ngram_preprocess(df)\n",
    "\n",
    "    \n",
    "#     #saving files\n",
    "#     df.apply(save_file,args = (base,),axis=1)\n",
    "\n",
    "#     c = datetime.datetime.now()\n",
    "    \n",
    "#     print(\"Running time, country: {}, punct_t: {}, text_proc_t: {}, total_t: {}\".format(keys, b-a, c-b, c-a))\n",
    "#     time_taken.append(c-a)\n",
    "\n",
    "# print(np.mean(time_taken))\n",
    "# print(np.sum(time_taken))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
